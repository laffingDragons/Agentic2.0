{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f851d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db61e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_API_KEY']=os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e0aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b45fd9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.033388253301382065,\n",
       " 0.034539785236120224,\n",
       " 0.05947456136345863,\n",
       " 0.05928615480661392,\n",
       " -0.06353536993265152,\n",
       " -0.06819583475589752,\n",
       " 0.08823321014642715,\n",
       " 0.034440815448760986,\n",
       " -0.032785218209028244,\n",
       " -0.015814967453479767,\n",
       " 0.02098170481622219,\n",
       " -0.01834026910364628,\n",
       " -0.03983220458030701,\n",
       " -0.0804707407951355,\n",
       " -0.014469261281192303,\n",
       " 0.0332648791372776,\n",
       " 0.014259261079132557,\n",
       " -0.034049976617097855,\n",
       " -0.1429157555103302,\n",
       " -0.023083392530679703,\n",
       " -0.02138015627861023,\n",
       " 0.002633563941344619,\n",
       " -0.04729270935058594,\n",
       " -0.010752730071544647,\n",
       " -0.0686679482460022,\n",
       " 0.031125007197260857,\n",
       " 0.07594592869281769,\n",
       " 0.0011282612103968859,\n",
       " 0.011631951667368412,\n",
       " -0.03603918105363846,\n",
       " 0.04483756422996521,\n",
       " 0.018390748649835587,\n",
       " 0.12672801315784454,\n",
       " -0.0013597434153780341,\n",
       " 0.0082066860049963,\n",
       " 0.06909967213869095,\n",
       " -0.08076360076665878,\n",
       " -0.05841311067342758,\n",
       " 0.05375456064939499,\n",
       " 0.02622753009200096,\n",
       " -0.006828601937741041,\n",
       " -0.05635838583111763,\n",
       " 0.0032929633744060993,\n",
       " -0.07250183075666428,\n",
       " 0.06960922479629517,\n",
       " 0.031674403697252274,\n",
       " -0.012384765781462193,\n",
       " 0.023199226707220078,\n",
       " 0.08131054788827896,\n",
       " 0.00027825479628518224,\n",
       " -0.12659244239330292,\n",
       " -0.04998631402850151,\n",
       " -0.0356525257229805,\n",
       " 0.04856063425540924,\n",
       " 0.0973307192325592,\n",
       " 0.06224411353468895,\n",
       " -0.03750477358698845,\n",
       " 0.008118405938148499,\n",
       " 0.027651365846395493,\n",
       " -0.04321661964058876,\n",
       " 0.01624852605164051,\n",
       " 0.002286759903654456,\n",
       " 0.0030970177613198757,\n",
       " -0.015322108753025532,\n",
       " 0.03742996230721474,\n",
       " -0.01050656195729971,\n",
       " -0.0532149039208889,\n",
       " -0.039694689214229584,\n",
       " -0.052877575159072876,\n",
       " -0.0304481890052557,\n",
       " -0.011697032488882542,\n",
       " 0.07245184481143951,\n",
       " -0.07213431596755981,\n",
       " 0.039102163165807724,\n",
       " -0.037163615226745605,\n",
       " 0.02642226405441761,\n",
       " 0.026734285056591034,\n",
       " -0.031146980822086334,\n",
       " 0.06340935081243515,\n",
       " -0.016946706920862198,\n",
       " 0.006343637593090534,\n",
       " -0.02548304945230484,\n",
       " -0.01599891483783722,\n",
       " 0.014684714376926422,\n",
       " -0.04103190824389458,\n",
       " 0.05668934807181358,\n",
       " 0.05189082399010658,\n",
       " 0.012766464613378048,\n",
       " 0.010396676138043404,\n",
       " 0.036034613847732544,\n",
       " -0.0748724415898323,\n",
       " 0.022285480052232742,\n",
       " 0.05366935580968857,\n",
       " 0.021017227321863174,\n",
       " 0.010703200474381447,\n",
       " 0.011209414340555668,\n",
       " 0.02216523513197899,\n",
       " -0.03350188583135605,\n",
       " -0.1379026621580124,\n",
       " 0.17661650478839874,\n",
       " 0.032913558185100555,\n",
       " 0.07648536562919617,\n",
       " -0.056457679718732834,\n",
       " 0.038201943039894104,\n",
       " -0.058867961168289185,\n",
       " 0.01618373394012451,\n",
       " -0.0029854567255824804,\n",
       " 0.008209971711039543,\n",
       " 0.02660369873046875,\n",
       " 0.03410131111741066,\n",
       " -0.0033713090233504772,\n",
       " -0.0468364953994751,\n",
       " 0.029342379420995712,\n",
       " -0.018165377900004387,\n",
       " 0.09393951296806335,\n",
       " 0.016673754900693893,\n",
       " -0.016232164576649666,\n",
       " 0.071003258228302,\n",
       " 0.041898343712091446,\n",
       " -0.012875045649707317,\n",
       " 0.027333272621035576,\n",
       " -0.03769176825881004,\n",
       " -0.010565726086497307,\n",
       " 0.0881001278758049,\n",
       " 0.033384472131729126,\n",
       " -0.0023000494111329317,\n",
       " -0.020081747323274612,\n",
       " -3.4389641919702815e-33,\n",
       " 0.003885326674208045,\n",
       " -0.045698072761297226,\n",
       " 0.03672377020120621,\n",
       " 0.1097324788570404,\n",
       " 0.005294252652674913,\n",
       " -0.0338403545320034,\n",
       " -0.0571880117058754,\n",
       " -0.030986450612545013,\n",
       " 0.028208503499627113,\n",
       " 0.014682860113680363,\n",
       " -0.005613868124783039,\n",
       " 0.028144465759396553,\n",
       " -0.06591302901506424,\n",
       " 0.016082216054201126,\n",
       " 0.0484490841627121,\n",
       " 0.07509639859199524,\n",
       " -0.004293183330446482,\n",
       " 0.026304572820663452,\n",
       " -0.02851446345448494,\n",
       " 0.022873923182487488,\n",
       " -0.0528033971786499,\n",
       " -0.0606650784611702,\n",
       " 0.019528530538082123,\n",
       " 0.05410182476043701,\n",
       " 0.02175348997116089,\n",
       " -0.025309894233942032,\n",
       " 0.031614772975444794,\n",
       " -0.12111617624759674,\n",
       " 0.049283333122730255,\n",
       " 0.002734116744250059,\n",
       " -0.038959406316280365,\n",
       " -0.02350739948451519,\n",
       " 0.010390670038759708,\n",
       " 0.02667590230703354,\n",
       " -0.020386051386594772,\n",
       " 0.013104500249028206,\n",
       " 0.012831229716539383,\n",
       " -0.027721155434846878,\n",
       " -0.07600447535514832,\n",
       " 0.03969910740852356,\n",
       " -0.06335610151290894,\n",
       " 0.0601436011493206,\n",
       " 0.052485156804323196,\n",
       " 0.008374616503715515,\n",
       " 0.015201447531580925,\n",
       " -0.018722597509622574,\n",
       " 0.012121781706809998,\n",
       " -0.014375097118318081,\n",
       " 0.0335976704955101,\n",
       " -0.013080411590635777,\n",
       " -0.06113576889038086,\n",
       " 0.02940642088651657,\n",
       " -0.09374036639928818,\n",
       " -0.0020576519891619682,\n",
       " 0.0240264181047678,\n",
       " -0.04306024685502052,\n",
       " -0.033853016793727875,\n",
       " -0.0036304888781160116,\n",
       " 0.005760233849287033,\n",
       " 0.01923433318734169,\n",
       " 0.024349495768547058,\n",
       " 0.07109120488166809,\n",
       " -0.04483497887849808,\n",
       " 0.07316726446151733,\n",
       " -0.08264467120170593,\n",
       " 0.005427990574389696,\n",
       " 0.023218726739287376,\n",
       " 0.06096756458282471,\n",
       " 0.0946073904633522,\n",
       " -0.014135166071355343,\n",
       " -0.041771139949560165,\n",
       " -0.027401773259043694,\n",
       " 0.03853410482406616,\n",
       " -0.007350060623139143,\n",
       " -0.007266113068908453,\n",
       " 0.04803266003727913,\n",
       " -0.01142668817192316,\n",
       " -0.06901122629642487,\n",
       " 0.0371512770652771,\n",
       " -0.06476709991693497,\n",
       " -0.06643080711364746,\n",
       " -0.0011766435345634818,\n",
       " -0.021795932203531265,\n",
       " -0.007523785345256329,\n",
       " 0.06725428253412247,\n",
       " 0.008094764314591885,\n",
       " -0.03811287507414818,\n",
       " -0.08759459853172302,\n",
       " 0.030362369492650032,\n",
       " -0.04203787446022034,\n",
       " -0.061076804995536804,\n",
       " 0.012958242557942867,\n",
       " 0.0050508202984929085,\n",
       " 0.018419792875647545,\n",
       " -0.12826375663280487,\n",
       " 2.624875946650291e-33,\n",
       " 0.0888892412185669,\n",
       " 0.032136183232069016,\n",
       " -0.10708092153072357,\n",
       " -0.01627512276172638,\n",
       " -0.041503164917230606,\n",
       " 0.002086916007101536,\n",
       " -0.06067205220460892,\n",
       " 0.12048248201608658,\n",
       " -0.08354712277650833,\n",
       " 0.050116825848817825,\n",
       " 0.0011807088740170002,\n",
       " -0.05077015236020088,\n",
       " 0.05996882915496826,\n",
       " 0.054858896881341934,\n",
       " 0.069722019135952,\n",
       " 0.0041571916081011295,\n",
       " 0.12240325659513474,\n",
       " 0.038177862763404846,\n",
       " -0.018991997465491295,\n",
       " 0.012684185989201069,\n",
       " -0.0325712189078331,\n",
       " 0.0266011543571949,\n",
       " -0.05980317294597626,\n",
       " -0.024400927126407623,\n",
       " -0.01654716208577156,\n",
       " 0.02027442306280136,\n",
       " -0.023540077731013298,\n",
       " 0.08794986456632614,\n",
       " -0.09462463855743408,\n",
       " -0.05664953961968422,\n",
       " 0.07793807238340378,\n",
       " -0.014540258795022964,\n",
       " -0.04598595201969147,\n",
       " 0.035977333784103394,\n",
       " 0.01722744293510914,\n",
       " 0.10854940861463547,\n",
       " 0.023995310068130493,\n",
       " -0.09443828463554382,\n",
       " 0.02173485979437828,\n",
       " -0.03514183685183525,\n",
       " -0.01556295994669199,\n",
       " -0.009056063368916512,\n",
       " -0.026420557871460915,\n",
       " 0.0981995090842247,\n",
       " -0.051732853055000305,\n",
       " -0.0511806383728981,\n",
       " -0.04413074627518654,\n",
       " 0.0607789047062397,\n",
       " -0.010921893641352654,\n",
       " -0.001620318042114377,\n",
       " -0.12291135638952255,\n",
       " -0.08137032389640808,\n",
       " -0.023045774549245834,\n",
       " -0.0507967509329319,\n",
       " -0.1166769340634346,\n",
       " 0.025221601128578186,\n",
       " -0.01985989511013031,\n",
       " 0.0496845468878746,\n",
       " -0.01369024720042944,\n",
       " 0.004341170657426119,\n",
       " -0.014368363656103611,\n",
       " 0.047707073390483856,\n",
       " 0.03718501701951027,\n",
       " 0.02282986044883728,\n",
       " -0.04654860496520996,\n",
       " 0.06936836242675781,\n",
       " -0.021325072273612022,\n",
       " 0.029570050537586212,\n",
       " 0.0009862696751952171,\n",
       " -0.033218372613191605,\n",
       " 0.041801657527685165,\n",
       " -0.0051827263087034225,\n",
       " -0.02064027637243271,\n",
       " 0.048608098179101944,\n",
       " -0.041332658380270004,\n",
       " -0.005364640615880489,\n",
       " -0.021895036101341248,\n",
       " 0.0212702676653862,\n",
       " 0.018460920080542564,\n",
       " -0.036591120064258575,\n",
       " -0.05728466808795929,\n",
       " -0.003445939626544714,\n",
       " -0.04625876620411873,\n",
       " 0.051345713436603546,\n",
       " 0.025934064760804176,\n",
       " 0.03859258443117142,\n",
       " 0.06607185304164886,\n",
       " -0.0173025019466877,\n",
       " -0.010598012246191502,\n",
       " 0.02265198528766632,\n",
       " 0.036723725497722626,\n",
       " 0.03584648668766022,\n",
       " 0.012036551721394062,\n",
       " 0.03178584948182106,\n",
       " -0.12105198949575424,\n",
       " -1.5130831698684233e-08,\n",
       " 0.006866039242595434,\n",
       " -0.027710292488336563,\n",
       " 0.08934707194566727,\n",
       " 0.07790021598339081,\n",
       " 0.06404797732830048,\n",
       " 0.03166112303733826,\n",
       " -0.07114408910274506,\n",
       " -0.04326837509870529,\n",
       " -0.03240783140063286,\n",
       " -0.02145582251250744,\n",
       " -0.0083053233101964,\n",
       " 0.03806024417281151,\n",
       " -0.00011355181777616963,\n",
       " 0.012233792804181576,\n",
       " 0.12013915181159973,\n",
       " 0.030236665159463882,\n",
       " -0.05191422626376152,\n",
       " -0.01144405733793974,\n",
       " -0.04784313961863518,\n",
       " -0.07834548503160477,\n",
       " 0.032210323959589005,\n",
       " -0.025861665606498718,\n",
       " -0.0020645514596253633,\n",
       " -0.02923078089952469,\n",
       " -0.01997469924390316,\n",
       " -0.03496574983000755,\n",
       " -0.014112574979662895,\n",
       " 0.03993988037109375,\n",
       " -0.06896211206912994,\n",
       " 0.05589042976498604,\n",
       " -0.012570615857839584,\n",
       " 0.1819690614938736,\n",
       " -0.012776501476764679,\n",
       " -0.0011513539357110858,\n",
       " 0.028124798089265823,\n",
       " 0.011322983540594578,\n",
       " 0.04628041386604309,\n",
       " 0.016169782727956772,\n",
       " 0.02629893273115158,\n",
       " -0.05404867231845856,\n",
       " -0.026807133108377457,\n",
       " 0.06764832884073257,\n",
       " 0.010659350082278252,\n",
       " -0.1292618066072464,\n",
       " 0.0693831816315651,\n",
       " 0.03072052076458931,\n",
       " 0.03887752443552017,\n",
       " -0.14078198373317719,\n",
       " 0.051911938935518265,\n",
       " -0.0679067075252533,\n",
       " -0.029856624081730843,\n",
       " 0.017362015321850777,\n",
       " 0.08236218988895416,\n",
       " 0.11180813610553741,\n",
       " 0.0989338830113411,\n",
       " 0.05719539895653725,\n",
       " 0.015903156250715256,\n",
       " -0.04091337323188782,\n",
       " -0.012436428107321262,\n",
       " 0.020249152556061745,\n",
       " 0.06743750721216202,\n",
       " 0.0393335297703743,\n",
       " 0.05208095535635948,\n",
       " -0.01940508931875229]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.embed_query(\"hello AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817e0328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"hello AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0af58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e8e1f2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\nPlease retry in 3.362462314s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'embedding-001', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/langchain_google_genai/embeddings.py:469\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/google/genai/models.py:4167\u001b[39m, in \u001b[36mModels.embed_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4165\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4167\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4168\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4171\u001b[39m response_dict = {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/google/genai/_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/google/genai/_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/google/genai/_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\nPlease retry in 3.362462314s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'embedding-001', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello AI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/langchain_google_genai/embeddings.py:476\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    475\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    478\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001\\nPlease retry in 3.362462314s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'embedding-001'}}, {'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'embedding-001', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}"
     ]
    }
   ],
   "source": [
    "embeddings.embed_query(\"Hello AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598953c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"Hello AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "#Serverless: Server will be Managed by the cloud provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319eb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"agenticbatch2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.has_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141ec0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.has_index(\"trading-bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a index\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ee18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the index\n",
    "index=pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=PineconeVectorStore(index=index,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da27854",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\"what is a langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d565b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},#additional info\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605eca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'tweet'}, page_content='I had chocolate chip pancakes and scrambled eggs for breakfast this morning.'),\n",
       " Document(metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
       " Document(metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\"),\n",
       " Document(metadata={'source': 'website'}, page_content='Is the new iPhone worth the price? Read this review to find out.'),\n",
       " Document(metadata={'source': 'website'}, page_content='The top 10 soccer players in the world right now.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
       " Document(metadata={'source': 'news'}, page_content='The stock market is down 500 points today due to fears of a recession.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :(')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35967003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257d8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "e395bddf-b420-4788-94d4-8f83c97728a3\n",
      "1\n",
      "ebea6cbd-9027-4f51-aa39-5e16c05e8a3c\n",
      "2\n",
      "162fca55-f473-44a5-b374-fed7bfbe35f7\n",
      "3\n",
      "0eb787de-4197-40e2-8f30-99b3508511b3\n",
      "4\n",
      "4b3220c6-6fa2-477f-83a9-ac31a5fd954a\n",
      "5\n",
      "61d59fb9-8617-40fb-a0ca-9bf3265cb8ca\n",
      "6\n",
      "ae21f1ec-de7a-455e-a69b-f8d0198860e2\n",
      "7\n",
      "773386b3-a158-498c-a6b5-74046a4ed53b\n",
      "8\n",
      "c8154df4-0143-475e-993a-f0d5b3e6315f\n",
      "9\n",
      "3dcf3b20-4673-4885-aca8-4da8a530f77e\n"
     ]
    }
   ],
   "source": [
    "for _ in range(len(documents)):\n",
    "    print(_)\n",
    "    print(str(uuid4()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#universal indentification number\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9675772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05a7e755-e4df-4b0a-9bca-09311fdd5ac2',\n",
       " '5046396c-4c6a-4e3e-a143-6a8b8c3f5931',\n",
       " '85d1f6a1-03c9-4355-931e-0c80c77cd0f0',\n",
       " 'b8447c1c-2bda-41ea-bf58-79384fb63c1e',\n",
       " '5a328412-3956-4f56-93f9-4c8f77e63b30',\n",
       " 'e7bfc231-871d-4924-99ee-e8405c9a7e9b',\n",
       " '02b1c490-bc53-4126-81ed-8163e2b9b300',\n",
       " '0f05bce0-3135-4fbb-969b-65ffa49644bd',\n",
       " 'ed5826fb-0503-4b5a-be60-0236f3e6e05e',\n",
       " 'b90492f0-82a3-4f45-8938-f1cd3c496249']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63eb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05a7e755-e4df-4b0a-9bca-09311fdd5ac2',\n",
       " '5046396c-4c6a-4e3e-a143-6a8b8c3f5931',\n",
       " '85d1f6a1-03c9-4355-931e-0c80c77cd0f0',\n",
       " 'b8447c1c-2bda-41ea-bf58-79384fb63c1e',\n",
       " '5a328412-3956-4f56-93f9-4c8f77e63b30',\n",
       " 'e7bfc231-871d-4924-99ee-e8405c9a7e9b',\n",
       " '02b1c490-bc53-4126-81ed-8163e2b9b300',\n",
       " '0f05bce0-3135-4fbb-969b-65ffa49644bd',\n",
       " 'ed5826fb-0503-4b5a-be60-0236f3e6e05e',\n",
       " 'b90492f0-82a3-4f45-8938-f1cd3c496249']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69962411",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\"what langchain provides to us?\",k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd992031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='85d1f6a1-03c9-4355-931e-0c80c77cd0f0', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\"what langchain provides to us?\",filter={\"source\": \"tweet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef1293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='85d1f6a1-03c9-4355-931e-0c80c77cd0f0', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
       " Document(id='0f05bce0-3135-4fbb-969b-65ffa49644bd', metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
       " Document(id='5a328412-3956-4f56-93f9-4c8f77e63b30', metadata={'source': 'tweet'}, page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\"),\n",
       " Document(id='05a7e755-e4df-4b0a-9bca-09311fdd5ac2', metadata={'source': 'tweet'}, page_content='I had chocolate chip pancakes and scrambled eggs for breakfast this morning.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c03bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7} #hyperparameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953de741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='85d1f6a1-03c9-4355-931e-0c80c77cd0f0', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
       " Document(id='0f05bce0-3135-4fbb-969b-65ffa49644bd', metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
       " Document(id='b90492f0-82a3-4f45-8938-f1cd3c496249', metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :('),\n",
       " Document(id='5a328412-3956-4f56-93f9-4c8f77e63b30', metadata={'source': 'tweet'}, page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\")]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba892cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='85d1f6a1-03c9-4355-931e-0c80c77cd0f0', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
       " Document(id='b90492f0-82a3-4f45-8938-f1cd3c496249', metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :('),\n",
       " Document(id='0f05bce0-3135-4fbb-969b-65ffa49644bd', metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
       " Document(id='e7bfc231-871d-4924-99ee-e8405c9a7e9b', metadata={'source': 'website'}, page_content='Is the new iPhone worth the price? Read this review to find out.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model=ChatGoogleGenerativeAI(model='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\agenticai-2.0\\env\\lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(prompt.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02dc2a9",
   "metadata": {},
   "source": [
    "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7473c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: what is a langchain? \\nContext: langchain is very super framework for LLM. \\nAnswer:\")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\":\"what is a langchain?\",\"context\":\"langchain is very super framework for LLM.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6f914",
   "metadata": {},
   "source": [
    "StringPromptValue(text=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: what is a langchain? \\nContext: langchain is very super framework for LLM. \\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61417850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55165cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"what is llama model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22974d64",
   "metadata": {},
   "source": [
    "# first complete the remaining part of this notebook(create a proper rag)\n",
    "\n",
    "\n",
    "\n",
    "# second assisgnment is: take a multiple pdf with text,image,table\n",
    "1. fetch the data from pdf\n",
    "2. at lesat there should be 200 pages\n",
    "3. if chunking(use the sementic chunking technique) required do chunking and then embedding\n",
    "4. store it inside the vector database(use any of them 1. mongodb 2. astradb 3. opensearch 4.milvus) ## i have not discuss then you need to explore\n",
    "5. create a index with all three index machnism(Flat, HNSW, IVF) ## i have not discuss then you need to explore\n",
    "6. create a retriever pipeline\n",
    "7. check the retriever time(which one is fastet)\n",
    "8. print the accuray score of every similarity search\n",
    "9. perform the reranking either using BM25 or MMR ## i have not discuss then you need to explore\n",
    "10. then write a prompt template\n",
    "11. generte a oputput through llm\n",
    "12. render that output over the DOCx ## i have not discuss then you need to explore\n",
    "as a additional tip: you can follow rag playlist from my youtube\n",
    "\n",
    "after completing it keep it on your github and share that link on my  mail id:\n",
    "snshrivas3365@gmail.com\n",
    "\n",
    "and share the assignment in your community chat as well by tagging krish and sunny\n",
    "\n",
    "deadline is: till firday 9PM\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af773942",
   "metadata": {},
   "source": [
    "saturday\n",
    "agentic session--> main session\n",
    "langgraph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
